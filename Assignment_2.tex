\documentclass{unswmaths}
\usepackage{unswshortcuts}
\usepackage{dsfont}
\usepackage{fullpage}
\usepackage{mdframed}
\usepackage{hyperref}
\begin{document}
\author{Adam J. Gray}
\title{Assignment 2}
\subject{Measure Theory}
\studentno{3329798}

\newcommand{\llra}{\Leftrightarrow}

\unswtitle

\section{}
\subsection{}
Let $ \mu $ and $ \nu $ be probability measures on $ ( \mathbb{R}, \mathcal{B}(\mathbb{R})) $.
We wish to show that the convolution
\begin{align}
	\mu * \nu(B) = \int\nu(B-x)\mu(dx)
\end{align}
of these two measures is well defined in the sense that $ \nu(B - x) $ is measurable in $ x $ and the integral exists.

Firstly see that
\begin{align}
	\nu(B-x) &= \int \chi_{B-x}(y) \nu(dy) \\
		&= \int \chi_{B}(x+y) \nu(dy)
\end{align}
and by seeing that $ \phi(x,y) = \chi_B(x+y) $ is $ \mathcal{B}(R) \otimes \mathcal{B}(R) $ measurable and so applying Tonelli's theorem $ \nu(B - x) $ is measurable.  

We now need to show that the integral exists. This is clear because
\begin{align}
	0 \leq \nu(B - x) \leq 1
\end{align}
and thus
\begin{align}
	\int \nu(B-x) \mu(dx) &\leq \int 1 \mu(dx) \\
		&= 1
\end{align}
because $ \mu $ is a probability measure. 

\subsection{}

Suppose there exists a bounded set $ F \in \mathcal{B}(\mathbb{R}) $ such that 
\begin{align}
	\mu * \nu(F) = 1 
\end{align}
we wish to show that there exists bounded sets $ G, H \in \mathcal{B}(\mathbb{R}) $ such that
\begin{align}
	\mu(G) = 1 \ \ \ \text{and} \ \ \ \nu(H) = 1. 
\end{align}
See that if 
\begin{align}
	1 &= \mu * \nu(F) \\
		&= \int \nu(F-x)\mu(dx) \\
		&= \int\int \chi_F(x+y) \nu(dy) \mu(dx)
\end{align}

Now as $ F $ is bounded there there exist intervals $ G = [a,b] $ and $ H = [c,d]$ such that
\begin{align}
	\int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy) \mu(dx)
\end{align}
and further
\begin{align}
	1=\int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) & \leq \int\int  \chi_G(x) \chi_H(y) \nu(dy) \mu(dx) \\
	&= \int \chi_{H}(y) \nu(dy) \int  \chi_G(x) \mu(dx)
\end{align}
\footnote{Note that the reason we could split up the integrals to a product in both cases is because $ \int \chi_H(y) \nu(y) $ is a constant with respect to $ x $.}
which implies that
\begin{align}
\int  \chi_G(x) \mu(dx) = 1 \ \ \ \text{ and } \ \ \ \int \chi_{H}(y) \nu(dy) = 1
\end{align}
that is $ \mu(G) = 1 $ and $ \nu(H) = 1 $.


Suppose that $ F $ is now countable but such that $ \mu * \nu (F) = 1 $.
Then
\begin{align}
	1 &= \int \nu(F - x) \mu(dx) \\
		&= \int \int \chi_{F}(x+y) \nu(dy) \mu(dx)
\end{align}
now since $ F = \left\{(x_k, y_k) \right\}_{k \in \Ntrl}$ is countable then there must exist countable sets $ G = \{ x_k \}_{k\in\Ntrl} $, $ H = \{ y_k \}_{k\in\Ntrl} $
such that
\begin{align}
	\int\int \chi_F(x+y) \nu(dy)\mu(dx) = \int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx)
\end{align}
and further
\begin{align}
	1 = \int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) &\leq \int\int \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) \\
	&= \int \chi_H(y) \nu(dy) \int \chi_G(x)\mu(dx)
\end{align}
which as before implies $ \mu(G) = 1 $ and $ \nu(H) = 1 $.

Then argument above also holds for finite $ F $ with exactly the same construction and so if $ F $ is finite and such that $ \mu * \nu(F) = 1 $ then there must exist finite sets $ G $ and $ H $ such that $ \mu(G) = 1 $ and $ \nu(H) = 1 $.
\section{}
\subsection{}
Suppose $ u $ and $ \nu $ are $\sigma$-finite \emph{positive} measures on $ (\Omega, \mathcal{F}) $. Then suppose that $ \mu << \nu $ and $ \nu << \mu $.
Then for $ A \in \mathcal{F} $ we have that $ \mu(a) = 0 \Rightarrow \nu(A) = 0 $ and $ \nu(A) = 0 \Rightarrow \mu(A) $. That is to say that $ \nu(A) =0 \Leftrightarrow \mu(A) = 0 $. That is to say that $ \nu $ and $ \mu $ have the same null sets. This argument is symmetric so it is clear that the reverse implication also holds.

We now wish to show that there is an $ \mathcal{F}$-measurable function $ g $ that satisfies $ 0 < g(\omega) < +\infty $ at each $ \omega \in \Omega $ and is such that $ \nu(A) = \int_Agd\mu $ for all $ A \in \mathcal{F} $.


Now suppose that there is a $ \mathcal{F} $ measurable function $ 0 < g(\omega) < \infty $ such that $ \nu(A) = \int_{A} g d\mu $.

Suppose $ \nu(A) =0 $ then
\begin{align}
	0 = \nu(A) &= \int_A g d\mu \\
	 &= \int \chi_A g d\mu
\end{align}
Now as $ \chi_A g >= 0 $ and $ \mu $ is a positive measure then by theorem 2.18 (3) we must that $ \chi_Ag = 0 $, $ \mu-$a.e. As $ g > 0 $ we have that $ \chi_A = 0 $ $ \mu$-a.e. which means that $ \mu(A) = 0 $.
So $ \mu \ll \nu $.

We claim that $ \nu \ll \mu $ as well. Suppose $ \mu(A) =0 $ then note that as $ g $ is a measurable function we have that by Theorem 2.11 $ g(\omega) = \lim_{n\lra\infty} s_n(\omega) $ for all $ \omega $ and a sequence of increasing simple functions.
That is
\begin{align}
	\int_A g d\mu = \int_A \lim_{n\lra\infty} s_n d\mu.
\end{align}
Further by the monotone convergence theorem we have that 
\begin{align}
\int_A \lim_{n\lra\infty} s_n d\mu = \lim_{n\lra\infty}\int_A s_n d\mu.
\end{align}

Now for some simple function $ s $ we have that if $ \mu(A) =0 $ then 
\begin{align}
	\int_A s d\mu &= \int_A\sum_{k=1}^{N} \alpha_k \chi_{B_k} d\mu \\
		&= \sum_{k=1}^N \alpha_k \int_A \chi_{B_k} d\mu \\
		&= \sum_{k=1}^N \alpha_k \int \chi_{B_k\cap A} d\mu \\
		&\leq  \sum_{k=1}^N \alpha_k \int \chi_{A} d\mu \\
		&= \sum_{k=1}^N \alpha_k 0\\
		&= 0
\end{align}
thus if $ \mu(A) = 0 $ then $ \int_A s_n d\mu =0 $ for all $ n $ and so $ \lim_{n \lra\infty} \int_A s_n d\mu = 0$ and hence $ \nu(A) =0 $. This means that $ \nu \ll \mu $ as well.

\subsection{}
Let $ \{ B_n \}_{n \in \Ntrl} $ be a covering of $ \Omega $ by disjoint sets with $ 0 < \mu(B_n) < \infty $ for all $ n $. Such a covering exists because $ \mu $ is $\sigma$-finite. Now select a sequence of constants $ \{ \alpha_n \}_{n \in \Ntrl} $ such that $ \alpha_n > 0 $ for all $ n $ and 
\begin{align}
	\sum_{n=1}^\infty \alpha_n = 1.
\end{align}
For example we could select  $ \alpha_n = 2^{-n} $.

We claim that the function $ \nu : \mathcal{F} \lra \mathbb{R} $ defined by
\begin{align}
	\nu(A) = \sum_{n=1}^\infty \alpha_n \frac{\mu(A \cap B_n)}{\mu(B_n)}
\end{align}
is a probability measure with the same null sets as $ \mu $.
Firstly we show that it is a measure, that is we show $\sigma$-additivity.

For a collection of disjoint sets $ \{ A_n \}_{n \in \Ntrl} $ we have that
\begin{align}
	\nu\left( \bigcup_{n\in\Ntrl} A_n \right) &= \sum_{k=1}^\infty \alpha_k \frac{\mu\left( \bigcup_{n=1}^\infty A_n \cap B_k \right)}{\mu(B_k)}
\end{align}
and by $ \sigma$-additivity of $ \mu $ we get
\begin{align}
	\sum_{k=1}^\infty \alpha_k \frac{\mu\left( \bigcup_{n=1}^\infty A_n \cap B_k \right)}{\mu(B_k)} &= \sum_{k=1}^\infty \alpha_k \frac{\sum_{n=1}^\infty \mu\left( A_n \cap B_k \right)}{\mu(B_k)} \\
	&= \sum_{n=1}^\infty  \sum_{k=1}^\infty \alpha_k \frac{\mu\left( A_n \cap B_k \right)}{\mu(B_k)} \\
	&= \sum_{n=1}^\infty \nu(A_n).
\end{align}
The interchange of the order of summation can be justified by the fact that
$ \mu(A_n \cap B_k) \geq 0 $ and 
\begin{align}
\sum_{k=1}^\infty \alpha_k \frac{\sum_{n=1}^\infty \mu\left( A_n \cap B_k \right)}{\mu(B_k)} < \infty
\end{align}
which we prove now (by proving $ \nu $ is a probability measure).

See that
\begin{align}
	\nu(\Omega) &= \sum_{k=1}^\infty \alpha_k \frac{\mu(\Omega\cap B_k)}{\mu(B_k)} \\
	&= \sum_{k=1}^\infty \alpha_k \frac{\mu( B_k)}{\mu(B_k)} \\
	&= \sum_{k=1}^\infty \alpha_k \\
	&= 1.
\end{align}
We now just have to show that $ \mu $ and $ \nu $ share the same null sets. 

Suppose $ \mu(A) = 0 $ then
\begin{align}
	\mu(A) &= \sum_{k=1}^\infty \alpha_k \frac{\mu(A \cap B_k)}{\mu(B_k)} \\
		&\leq \sum_{k=1}^\infty \alpha_k \frac{\mu(A)}{\mu(B_k)} \\
		&= 0
\end{align}
and thus $ \nu \ll \mu $.
Now suppose $ \nu(A) =0 $ then
\begin{align}
	\sum_{k=1}^\infty \alpha_k \frac{\mu(A \cap B_k)}{\mu(B_k)} = 0
\end{align}
and as $ \mu(B_k) < \infty $ for all $ k $ this implies that $ \mu(A \cap B_k ) = 0$ for all $ k $. Now
\begin{align}
	0 &= \sum_{k=1}^\infty \mu(A \cap B_k) \\
		&= \mu\left( \bigcup_{k=1}^\infty (A \cap B_k) \right) \\
		&= \mu\left( A \cap \bigcup_{k=1}^\infty B_k\right) \\
		&= \mu(A \cap \Omega) \\
		&= \mu(A).
\end{align}
Thus $ \mu \ll \nu $.

So $ \nu $ is a finite measure on $ (\Omega, \mathcal{F}) $ which is equivalent to $ \mu $.
\section{}
\subsection{}
Firstly note that the definition of the characteristic function is
\begin{align}
	\hat{\mu}_X(u) = \mathbb{E}[\exp(i\langle X, u \rangle)]
\end{align}
and so for the random vector $ cX $ with $ c \in \Rl $ we have that
\begin{align}
	\hat{\mu}_{cX}(u) &= \mathbb{E}[ \exp(i \langle cX, u \rangle) ] \\
		 &= \mathbb{E}[ \exp(i \langle X, cu \rangle) ] \\
		 &= \hat{\mu}_{X}(cu)
\end{align}
\subsection{}
By definition we have that
\begin{align}
	\hat{\mu}(\mathbf{u}) = \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We can say that
\begin{align}
	\frac{d^\alpha \hat{\mu}(\mathbf{u})}{d \mathbf{u}^\alpha} = \frac{d^\alpha}{d \mathbf{u}^\alpha} \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We wish to justify taking this derivative through the integral sign. To do this we use an extension of corollary 2.28 (2) from the notes.
\clearpage
\begin{mdframed}
\textbf{Claim}

If the partial derivative 
\begin{align}
	\frac{\partial^\alpha}{\partial\mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u})
\end{align}
exists for all $ (\mathbf{x}, \mathbf{u}) \in X \times [a, b]^d $ and if there is a function $ g \in L^{1}(\mu) $ such that
\begin{align}
	\Big| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) \Big| \leq g
\end{align}
for every $ \mathbf{x} \in X $ and $ \mathbf{u} \in (a,b)^d $ then
\begin{align}
	\frac{d^\alpha}{d\mathbf{u}^\alpha} \int f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) = \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d
\end{align}
\textbf{Proof}

We induct on $ |\alpha| $. For $ |\alpha| = 0 $ the result is degenerate.
Suppose it is true for $ |\alpha| $ we wish to show that it is true for $ |\alpha| + 1 $. 
If it is true for $ |\alpha| $ then we have that
\begin{align}
	\frac{d^\alpha}{d\mathbf{u}^\alpha} \int f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) = \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d
\end{align}
then for $ |\alpha| + 1 $ with a derivative with respect to the $ ith $ component we would have
\begin{align}
	\frac{d}{du_i}\frac{d^\alpha}{d\mathbf{u}^\alpha} \int f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) = \frac{d}{du_i}\int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d
\end{align}
but by corollary 2.28 (2) we have
\begin{align}
	\frac{d}{du_i}\int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x})
 	&= \int \frac{\partial }{\partial u_k} \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x})\\
 	&= \int \frac{\partial^{\alpha+1}}{\partial \mathbf{u}^{\alpha+1}} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d
\end{align}
and so we have the result by induction. \footnote{We abused notation with $ |\alpha| + 1 $ and $ \alpha + 1 $ but by $ \alpha + 1 $ we mean we increase the order of the derivative of the $ ith $ variable. Also note that we assume that we can interchange the orders of differentiation (or combine them). This is only allowed if $ f $ has continuous partial derivatives of the appropriate order. For this case we assume this is ok.} 
\end{mdframed}

We have that 
\begin{align}
	\frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) = i^{|\alpha|}\prod_{k=1}^d x_k^{\alpha_k}  \exp(i \langle \mathbf{x}, \mathbf{u} \rangle )
\end{align}
and that
\begin{align}
	\left| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) \right| \leq  \underbrace{\prod_{k=1}^d \left| x_k \right|
^{\alpha_k}}_{\circledast}.
\end{align}
Now because 
\begin{align}
	\mathbf{E}\left( \prod_{k=1}^d |X_k|^{\alpha_k}\right) = \int \prod_{k=1}^d \left| x_k \right|
^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) < \infty 
\end{align}
then $ \circledast \in L^1(\mathbb{P}_X) $
and so we can apply our claim (the DCT) to get
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} &=  \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= \int i^{|\alpha|} \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x})
\end{align}
and so
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} \Big|_{\mathbf{u} = \mathbf{0}} &= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{0} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \mathbb{E}(X^\alpha)
\end{align}
\subsection{}
Notice that as the distribution is symmetric we have that
\begin{align}
	\hat{\mu}(u) &= \int e^{iux} f(x) dx \\
		 &= \int f(x)( \cos(ux) + i\sin(ux)) dx\\
		 &= \int f(x) cos(ux) dx
\end{align}
Now notice that
\begin{align}
	\left|\frac{\partial}{\partial u} f(x) \cos(ux)\right| = |xf(x)sin(xu)|
\end{align}
Now notice that
\begin{align}
	\int |xf(x)\sin(ux)| dx = \underbrace{\int_{-K}^{K} |xf(x)\sin(ux)| dx}_{\leq M_1} + \int_{-\infty}^{-K} |xf(x)\sin(ux)| dx +\int_{K}^{\infty} |xf(x)\sin(ux)| dx
\end{align}
for some constant $ M_1 $ and a constants $ K $ and $ J $ chosen such that
\begin{align}
	\int_{K}^\infty |xf(x)\sin(ux)| dx \leq \int_{K}^\infty \frac{|\sin(ux)|}{Jx} dx.
\end{align}
We can therefore say that
\begin{align}'
	\int |xf(x)\sin(ux)|dx \leq M_1 + 2 \int_K^\infty \frac{|\sin(ux)|}{Jx}dx.
\end{align}
Now appealing to a result in special functions we can say that 
\begin{align}
	\int_K^\infty \frac{|\sin(ux)|}{Jx}dx = \lim_{x \lra \infty} \operatorname{Si}(ux)\operatorname{sgn}(\sin(ux)) -\operatorname{Si}(uK)\operatorname{sgn}(\sin(uK))
\end{align}
and it turns out that $ \frac{1}{J}\lim_{x\lra\infty} \operatorname{Si}(ux)\operatorname{sgn}(\sin(ux)) < \infty $ so long as $ u \neq 0 $ which means that 
\begin{align}
	\left|\frac{\partial}{\partial u} f(x) \cos(ux)\right| \in L^1(\mu)
\end{align}
for every $ u \neq 0 $. 
We can then apply the dominated convergence theorem corollary 2.28 (2) to say that $ \hat{\mu}(u) $ is differentiable for all $ u \neq 0 $ and by extension, sufficiently close to $ 0 $.
\section{}
Let $\mu$ be the binomial distribution with $ n $ trials and probability of success $ p $, that is $ \mu = \operatorname{Bin}(n,p) $, and let $ \nu $ be the Poisson distribution with mean $ \lambda > 0 $. 
\subsection{}
We wish to verify that $ \hat{\mu}(u) = (1 - p + pe^{iu})^n $. 
Because the binomial distribution is just the convolution of identical independent Bernoulli distributions then we just have to verify that $ (1 - p + pe^{iu} $ is the characteristic function for $Bernoulli(p)$.

If $ \nu $ is the Bernoulli measure and $ X $ has law $ \nu $ then 
\begin{align}
	\hat{\nu}(u) &= \mathbb{E}[ \exp(iuX)] \\
		&= \sum_{k \in \{ 0, 1\}} e^{iuk} \nu_{X}(k) \\
		&=  pe^{iu} + (1 - p).
\end{align}
Then by repeated application of the convolution theorem we get that
$ \hat{\mu}(u) = (1 - p + pe^{iu})^n $.

\subsection{}
We wish to verify that $ \hat{\nu}(u) = \exp(\lambda(e^{iu} - 1)) $. 
The probability mass function of the Poisson distribution is 
\begin{align}
	\frac{\lambda^k}{k!} e^{-\lambda} 
\end{align}
and thus
\begin{align}
	\mathbb{E}[ \exp(iuX) ] &= \sum_{k=0}^\infty \frac{\lambda^k}{k!} e^{-\lambda} e^{iuk} \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!} (e^{iu})^k \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{(\lambda e^{iu})^k}{k!} \\
		&= e^{-\lambda} e^{\lambda e^{iu}} \\
		&= e^{\lambda( e^{iu} - 1)}
\end{align}

\subsection{}

We wish to show that if $ p_n $ is a sequence in $ [0,1] $ such that $ p_n \downarrow 0 $ and $ np_n \lra \lambda $ then $ \mu_n \lra \nu $ in the weak sense where $ \mu_n = \operatorname{Bin}(n, p_n) $.
Let $ f \in C_b $ then
\begin{align}
	\lim_{n\lra \infty} \sum_{k=0}^{n} f(k) {n\choose k} p_n^k(1-p_n)^{n-k}
		&= \lim_{n\lra \infty} \sum_{k=0}^\infty \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} \\
		&= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k}.
\end{align}
The interchange of the order of the limit and the sum is justified by the uniform convergence of the sum. To see this let $ M = \sup_{k \in \Ntrl^0} f(k) $ (which exists because $ f \in C_b $) and then note that 
\begin{align}
	\sum_{k=0}^\infty \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} &\leq \sum_{k=0}^\infty \chi_{k \leq n} \cdot M {n\choose k} p_n^k(1-p_n)^{n-k} \\
	&= M < \infty
\end{align}
and so by the Weierstrass M test the series converges uniformly. 

Now as $ np_n \lra \lambda $ or $ p_n \lra \frac{\lambda}{n} $ we get
\begin{align}
	\sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} &= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) \frac{n!}{k!(n-k)!} p_n^k(1-p_n)^{n} (1-p_n)^{-k} \\
	&= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) \frac{n^k + O(n^{k-1})}{k!} p_n^k(1-p_n)^{n} (1-p_n)^{-k} \\
	&= \sum_{k=0}^\infty  f(k)\lim_{n\lra \infty} \frac{n^k + O(n^{k-1})}{k!} p_n^k(1-p_n)^{n} \underbrace{(1-p_n)^{-k}}_{\lra 0} \\
	&= \sum_{k=0}^\infty  f(k)\lim_{n\lra \infty} \underbrace{\frac{n^k + O(n^{k-1})}{k!} p_n^k}_{\lra \frac{\lambda^k}{k!}}(1-p_n)^{n} \\
	&= \sum_{k=0}^\infty \frac{\lambda^k}{k!} f(k)\lim_{n\lra \infty} \underbrace{(1-\frac{\lambda}{n})^{n}}_{\lra e^{-\lambda}} \\
	&= \sum_{k=0}^\infty \frac{\lambda^k}{k!} e^{-\lambda} f(k).
\end{align}
This proves the weak convergence.
\subsection{}
This argument holds whether one takes the integral (sum) or not. So $ \mu_n(\{k\}) \lra \nu(\{k\}) $ for all $ k \in \Ntrl^0$.
\section{}
\subsection{}
We wish to show that $ \mathbb{P}(B_n) = 1/2 $ for every $ n \geq 1 $.
Note that 
\begin{align}
	B_n = \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big)
\end{align}
and thus
\begin{align}
	\mathbb{P}(B_n) &= \mathbb{P}\left( \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \mathbb{P}\left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \frac{1}{2^n} \\
		&= 2^{n-1} \frac{1}{2^n} \\
		&= \frac{1}{2}.
\end{align}
\subsection{}
We now wish to show that the sequence of events $ B_n $ form an infinite sequence of independent events. 

Take a finite subset $ J \subset \Ntrl $ with $ |J| = m $ and $ \max J = r $ then
\begin{align}
	\mathbb{P}\left( \bigcap_{n \in J} B_n \right) &= \mathbb{P}\left( \bigcap_{n \in J} \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= \mathbb{P} \left( \bigcup_{k=0}^{2^{r-m} - 1} \Big[ \frac{2k}{2^r}, \frac{2k+1}{2^r} \Big)\right) \\
	&=  \sum_{k=0}^{2^{r-m} - 1} \mathbb{P} \left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= 2^{r-m} \frac{1}{2^r} \\
	&= \frac{1}{2^m} \\
	&= \prod_{n \in J} \mathbb{P}\left( B_n \right)
\end{align}
and so the sequence of events $ B_n $ form an infinite sequence of independent events.
\subsection{}

We wish to show / argue that the probability that a randomly sampled number $ \omega $ will have the sequence $ 5825 $ occur infinitely often in its decimal expansion is 1.

We use the Borel-Cantelli lemma. Ignoring possible overlaps (on the $5$s) we can see that we can break any decimal expansion of $ \omega $ up into blocks of $ 4 $ digits. 

Then by we can define $ E_i $ as the probability of obtaining $ 5285 $ in the i-th block possition. By the same argument as above these events are independent.

The for any $ i $ we have that $ \mathbb{P}(E_i) = \frac{1}{10000} $ (the same argument as above applied to a decimal expansion). Then clearly
\begin{align}
  \mathbb{P}( E_i ) = \infty.
\end{align}

By the Borel-Cantelli lemma this implies
\begin{align}
  \mathbb{P}( \limsup_n( E_n ) = 1.
\end{align}

Now 
\begin{align}
  \limsup_n( E_n ) = \bigcap_{n=1}^\infty \bigcup_{j=n}^\infty E_j 
\end{align}
can be intuitively read as $ E_j $ happens infinitely often. Which is to say that $ 5285 $ occurs \emph{blockwise} in the expansion of $ \omega $ infinitely often. Clearly as allowing for overlaps allows for more configurations then the probability is $ 1 $ (it can be no more). 

\section{}
We wish to prove that if $ A $ and $ B $ are i.i.d random variables with finite variance and $ A+B $ and $ A-B $ are independent then $ A $ and $ B $ are normal. This result is actually called Bernstein's Theorem. (Actually there are lots of things called Bernstein's Theorem, but this is one of them.)\footnote{This proof is based off ideas from 

\url{http://math.stackexchange.com/questions/556030/x-and-y-i-i-d-xy-and-x-y-independent-mathbbex-0-and-mathbb}}.

Firstly write $ X = (A - \mathbb{E}(A))/\sqrt{\mathbb{E}(A^2)} $ and $ Y = (B - \mathbb{E}(B))/\sqrt{\mathbb{E}(B^2)} $. $ \mathbb{E}(A) $, $ \mathbb{E}(A^2) $, $\mathbb{E}(B) $ and $\mathbb{E}(B^2) $  exist because $ \operatorname{Var}(A) < \infty $ and $ \operatorname{Var}(B) < \infty $ .

Notice that $ \mathbb{E}(X) = 0 $ and $ \mathbb{E}(X^2) = 1 $.

Notice that we can write 
\begin{align}
	X = \frac{1}{2}((X+Y)+(X-Y)).
\end{align}
So if we write $ \hat{\mu_X}(u) $, $ \hat{\mu_Y}(u) $,$ \hat{\mu_{X+Y}}(u) $ and $ \hat{\mu_{X-Y}}(u)$ as the characteristic functions of $ X $, $ Y $, $X+Y $ and $ X-Y$ respectively we can write.
\begin{align}
	\hat{\mu_X}(u) =  \hat{\mu_{X+Y}}(\frac{u}{2}) \hat{\mu_{X-Y}}(\frac{u}{2})
\end{align}
because $ X+Y $ and $ X-Y $ are independent.
Now because $ X $ and $ Y $ are independent we can further write
\begin{align}
\hat{\mu_X}(u) = \hat{\mu_X}(\frac{u}{2})^2 \hat{\mu_Y}(u/2)\hat{\mu_Y}(-\frac{u}{2}).
\end{align}
Now as $ X $ and $ Y $ are identically distributed they have the same characteristic function and so we can say $ \hat{\mu_X} \equiv \hat{\mu_Y} $ and hence from now on we just write $ \hat{\mu} $.
\begin{align}
	\hat{\mu}(u) = \hat{\mu}(\frac{u}{2})^3\hat{\mu}(-\frac{u}{2}).
\end{align}
Lets define $ \Psi(u) = \log(\hat{\mu}(u)) $. In this case the functional equation reduces to
\begin{align}
	\Psi(u) = 3 \Psi(\frac{u}{2}) + \Psi(-\frac{u}{2})
\end{align}
and
\begin{align}
	\Psi(-u) = 3 \Psi(-\frac{u}{2}) + \Psi(\frac{u}{2})
\end{align}
Now by defining $ \Xi(u) = \Psi(u) - \Psi(-u) $. We can reduce the functional equation to  $ \Xi(u) = 2\Xi(\frac{u}{2})$. Then
\begin{align}
	\Xi(\frac{u}{2^n}) = 2^n \Xi(u)
\end{align}
and by rearranging and dividing by $ t $ we get 
\begin{align}
	\frac{\Xi(u)}{u} = \frac{\Xi(\frac{u}{2^n})}{\frac{u}{2^n}}.
\end{align}
Now if $ \mathbb{E}[X] = \alpha $ and $ \mathbb{E}[X^2] = \beta $ (these values exist because of the assumption of finite variance) we get that
$ \hat{\mu}'(0)= 0 $ and $ \hat{\mu}''(0) = -1 $ by the results of 3.2 of this assignment. Now as $ \hat{\mu} $ is twice differentiable then so are $ \Xi $ and $ \Psi $.
Now $ \Xi'(u) = 2\Psi'(u) $ and $ \Psi'(u) = \frac{\hat{\mu}'(u)}{\hat{\mu}(u)} $. Thus $ \Xi'(0) = 0 $ and since
\begin{align}
	\frac{\Xi(u)}{u} =\frac{\Xi(\frac{u}{2^n})}{\frac{u}{2^n}}
\end{align}
and 
\begin{align}
	\lim_{n\lra\infty} \frac{\Xi(\frac{u}{2^n})}{\frac{u}{2^n}} = \Xi'(0)
\end{align}
and thus $ \Xi[u] = i\alpha u $. 

Now 
\begin{align}
 \Psi''(u) = \frac{\hat{\mu}''(u) \hat{\mu}(u) - (\hat{\mu}'{(u)})^2}{\hat{\mu}(u)^2}
 \end{align}
 and at zero this means $ \Psi''(0) = -1 $.
 By using the previous result for $ \Xi'[u] = 0 $ along with its definition we have that
 \begin{align}
 	\Psi(u) = 4\Psi(\frac{u}{2})
 \end{align}
 and so 
 \begin{align}
 	\frac{\Psi(u)}{u^2} = \frac{4}{\frac{u^2}{4^n}}\Psi(\frac{u}{2^n}).
 \end{align}
 Now
 \begin{align}
 	\lim_{n\lra\infty}\frac{4}{\frac{u^2}{4^n}}\Psi(\frac{u}{2^n}) = \frac{1}{2} \Psi''(0)
 \end{align}
 through one application of L'Hopitals rule.
 
 Thus $ \Psi(u) = -\frac{1}{2}(u^2) $
 which means that $ \hat{\mu}(u) = \exp(-\frac{1}{2}(u^2)) $.
 Notice that this is the characteristic function of a standard normal distribution and so $ X $ and $ Y $ both have standard normal distributions. By rescaling we get that $ A $ and $ B $ are also normally distributed (just with different parameters).
 
 It is not hard to see that this result generalises to larger collections of random variables. For a proof of that result see \emph{Lukacs, Eugene; King, Edgar P. (1954). ``A Property of Normal Distribution'' The Annals of Mathematical Statistics 25 (2): 389 \textemdash 394.}
\end{document}