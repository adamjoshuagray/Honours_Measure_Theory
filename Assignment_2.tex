\documentclass{unswmaths}
\usepackage{unswshortcuts}
\usepackage{dsfont}
\usepackage{fullpage}
\usepackage{mdframed}
\begin{document}
\author{Adam J. Gray}
\title{Assignment 2}
\subject{Measure Theory}
\studentno{3329798}

\newcommand{\llra}{\Leftrightarrow}

\unswtitle

\section{}
\section{}
\subsection{}
Suppose $ u $ and $ \nu $ are $\sigma$-finite \emph{positive} measures on $ (\Omega, \mathcal{F}) $. Then suppose that $ \mu << \nu $ and $ \nu << \mu $.
Then for $ A \in \mathcal{F} $ we have that $ \mu(a) = 0 \Rightarrow \nu(A) = 0 $ and $ \nu(A) = 0 \Rightarrow \mu(A) $. That is to say that $ \nu(A) =0 \Leftrightarrow \mu(A) = 0 $. That is to say that $ \nu $ and $ \mu $ have the same null sets. This argument is symmetric so it is clear that the reverse implication also holds.

We now wish to show that there is an $ \mathcal{F}$-measurable function $ g $ that satisfies $ 0 < g(\omega) < +\infty $ at each $ \omega \in \Omega $ and is such that $ \nu(A) = \int_Agd\mu $ for all $ A \in \mathcal{F} $.

\section{}
\subsection{}
Firstly note that the definition of the characteristic function is
\begin{align}
	\hat{\mu}_X(u) = \mathbb{E}[\exp(i\langle X, u \rangle)]
\end{align}
and so for the random vector $ cX $ with $ c \in \Rl $ we have that
\begin{align}
	\hat{\mu}_{cX}(u) &= \mathbb{E}[ \exp(i \langle cX, u \rangle) ] \\
		 &= \mathbb{E}[ \exp(i \langle X, cu \rangle) ] \\
		 &= \hat{\mu}_{X}(cu)
\end{align}
\subsection{}
By definition we have that
\begin{align}
	\hat{\mu}(\mathbf{u}) = \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We can say that
\begin{align}
	\frac{d^\alpha \hat{\mu}(\mathbf{u})}{d \mathbf{u}^\alpha} = \frac{d^\alpha}{d \mathbf{u}^\alpha} \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We wish to justify taking this derivative through the integral sign. To do this we use an extension of corollary 2.28 (2) from the notes.
\clearpage
\begin{mdframed}
\textbf{Claim}

If the partial derivative 
\begin{align}
	\frac{\partial^\alpha}{\partial\mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u})
\end{align}
exists for all $ (\mathbf{x}, \mathbf{u}) \in X \times [a, b]^d $ and if there is a function $ g \in L^{1}(\mu) $ such that
\begin{align}
	\Big| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) \Big| \leq g
\end{align}
for every $ \mathbf{x} \in X $ and $ \mathbf{u} \in (a,b)^d $ then
\begin{align}
	\frac{d^\alpha}{d\mathbf{u}^\alpha} \int f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) = \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d.
\end{align}
The proof follows from induction on the dimension of $ \mathbf{x} $ and $ \mathbf{u} $ and the order of the derivative. The base case is specifically the statement of  corollary 2.28 (2) from the notes.
\end{mdframed}

We have that 
\begin{align}
	\frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) = i^{|\alpha|}\prod_{k=1}^d x_k^{\alpha_k}  \exp(i \langle \mathbf{x}, \mathbf{u} \rangle )
\end{align}
and that
\begin{align}
	\left| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) \right| \leq  \underbrace{\prod_{k=1}^d \left| x_k \right|
^{\alpha_k}}_{\circledast}.
\end{align}
Now because 
\begin{align}
	\mathbf{E}\left( \prod_{k=1}^d |X_k|^{\alpha_k}\right) = \int \prod_{k=1}^d \left| x_k \right|
^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) < \infty 
\end{align}
then $ \circledast \in L^1(\mathbb{P}_X) $
and so we can apply our claim (the DCT) to get
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} &=  \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= \int i^{|\alpha|} \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x})
\end{align}
and so
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} \Big|_{\mathbf{u} = \mathbf{0}} &= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{0} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \mathbb{E}(X^\alpha)
\end{align}
\subsection{}
Let $ d = 1 $ and let $ \mu $ have the  Lebesgue density,
\begin{align}
	f(x) = \frac{C}{(1+x^2) \log(e + x^2)}, \ \ \ \ x \in \Rl.
\end{align}
We wish to show that $ E[X] $ is not defined but $ \hat{\mu}(u) $ is differentiable at $ 0 $. Firstly we show that $ E[X] $ is not defined. 

\begin{align}
	E[X] &= \int_{-\infty}^\infty \frac{Cx}{(1+x^2) \log(e + x^2)} dx \\
		&= \int_{-\infty}^\infty \frac{C}{(x^{-1} + x)\log(e + x^2)}
\end{align}
note that
\begin{align}
	\frac{C}{(x^{-1} + x)\log(e + x^2)} \sim \frac{C}{2x\log(x)}
\end{align}
and 
\begin{align}
	\int_{a}^\infty \frac{C}{2x\log(x)}dx, \ \text{and} \ \int_{-\infty}^b \frac{C}{2x\log(x)}dx
\end{align}
do not converge so $ \mathbb{E}[X] $ does not exist.

We now just have to show that $ \hat{mu}(u) $ is differentiable at $ u = 0 $. 

\begin{align}
	\hat{\mu}(u) = \int_{-\infty}^\infty \frac{e^{iux}C}{(1+x^2)\log(e+x^2)} dx
\end{align}
and
\begin{align}
	\frac{d}{du}\hat{\mu}(u) \Big|_{u = 0} = \int_{-\infty}^\infty \frac{ixe^{iux}C}{(1+x^2)\log(e+x^2)} dx
\end{align}

\section{}
Let $\mu$ be the binomial distribution with $ n $ trials and probability of success $ p $, that is $ \mu = \operatorname{Bin}(n,p) $, and let $ \nu $ be the Poisson distribution with mean $ \lambda > 0 $. 
\subsection{}
We wish to verify that $ \hat{\mu}(u) = (1 - p + pe^{iu})^n $. 
Because the binomial distribution is just the convolution of identical independent Bernoulli distributions then we just have to verify that $ (1 - p + pe^{iu} $ is the characteristic function for $Bernoulli(p)$.

If $ \nu $ is the Bernoulli measure and $ X $ has law $ \nu $ then 
\begin{align}
	\hat{\nu}(u) &= \mathbb{E}[ \exp(iuX)] \\
		&= \sum_{k \in \{ 0, 1\}} e^{iuk} \nu_{X}(k) \\
		&=  pe^{iu} + (1 - p).
\end{align}
Then by repeated application of the convolution theorem we get that
$ \hat{\mu}(u) = (1 - p + pe^{iu})^n $.

\subsection{}
We wish to verify that $ \hat{\nu}(u) = \exp(\lambda(e^{iu} - 1)) $. 
The probability mass function of the Poisson distribution is 
\begin{align}
	\frac{\lambda^k}{k!} e^{-\lambda} 
\end{align}
and thus
\begin{align}
	\mathbb{E}[ \exp(iuX) ] &= \sum_{k=0}^\infty \frac{\lambda^k}{k!} e^{-\lambda} e^{iuk} \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!} (e^{iu})^k \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{(\lambda e^{iu})^k}{k!} \\
		&= e^{-\lambda} e^{\lambda e^{iu}} \\
		&= e^{\lambda( e^{iu} - 1)}
\end{align}

\subsection{}

We wish to show that if $ p_n $ is a sequence in $ [0,1] $ such that $ p_n \downarrow 0 $ and $ np_n \lra \lambda $ then $ \mu_n \lra \nu $ in the weak sense where $ \mu_n = \operatorname{Bin}(n, p_n) $.

Let $ f \in C_b $ then 
\begin{align}
	E[f(X_n)] &= \sum_{k=0}^n f(k) \mu_n(k) \\
		&= \sum_{k=0}^n f(k) \mu_n(k) \\
		&= \sum_{k=0}^n f(k) {n\choose k} p_n^k(1-p_n)^{n-k}
\end{align}
then
\begin{align}
	\lim_{n\lra\infty} E[f(X_n)] &= \lim_{n \lra \infty}\sum_{k=0}^n f(k) {n\choose k} p_n^k(1-p_n)^{n-k}
\end{align}
and
\begin{align}
	\mathbb{E}[f(Y)] &= f(x) \nu(x) \\
		&= \sum_{k=0}^\infty f(k) \frac{\lambda^k}{k!} e^{-\lambda} \\
\end{align}

\section{}
\subsection{}
We wish to show that $ \mathbb{P}(B_n) = 1/2 $ for every $ n \geq 1 $.
Note that 
\begin{align}
	B_n = \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big)
\end{align}
and thus
\begin{align}
	\mathbb{P}(B_n) &= \mathbb{P}\left( \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \mathbb{P}\left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \frac{1}{2^n} \\
		&= 2^{n-1} \frac{1}{2^n} \\
		&= \frac{1}{2}.
\end{align}
\subsection{}
We now wish to show that the sequence of events $ B_n $ form an infinite sequence of independent events. 

Take a finite subset $ J \subset \Ntrl $ with $ |J| = m $ and $ \max J = r $ then
\begin{align}
	\mathbb{P}\left( \bigcap_{n \in J} B_n \right) &= \mathbb{P}\left( \bigcap_{n \in J} \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= \mathbb{P} \left( \bigcup_{k=0}^{2^{r-m} - 1} \Big[ \frac{2k}{2^r}, \frac{2k+1}{2^r} \Big)\right) \\
	&=  \sum_{k=0}^{2^{r-m} - 1} \mathbb{P} \left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= 2^{r-m} \frac{1}{2^r} \\
	&= \frac{1}{2^m} \\
	&= \prod_{n \in J} \mathbb{P}\left( B_n \right)
\end{align}
and so the sequence of events $ B_n $ form an infinite sequence of independent events.
\subsection{}

We wish to show / argue that the probability that a randomly sampled number $ \omega $ will have the sequence $ 5825 $ occur infinitely often in its decimal expansion is 1.

We use the Borel-Cantelli lemma. Ignoring possible overlaps (on the $5$s) we can see that we can break any decimal expansion of $ \omega $ up into blocks of $ 4 $ digits. 

Then by we can define $ E_i $ as the probability of obtaining $ 5285 $ in the i-th block possition. By the same argument as above these events are independent.

The for any $ i $ we have that $ \mathbb{P}(E_i) = \frac{1}{10000} $ (the same argument as above applied to a decimal expansion). Then clearly
\begin{align}
  \mathbb{P}( E_i ) = \infty.
\end{align}

By the Borel-Cantelli lemma this implies
\begin{align}
  \mathbb{P}( \limsup_n( E_n ) = 1.
\end{align}

Now 
\begin{align}
  \limsup_n( E_n ) = \bigcap_{n=1}^\infty \bigcup_{j=n}^\infty E_j 
\end{align}
can be intuatively read as $ E_j $ happens infinitely often. Which is to say that $ 5285 $ occurs \emph{blockwise} in the expansion of $ \omega $ infinitely often. Clearly as allowing for overlaps allows for more configurations then the probability is $ 1 $ (it can be no more). 
\end{document}