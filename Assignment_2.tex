\documentclass{unswmaths}
\usepackage{unswshortcuts}
\usepackage{dsfont}
\usepackage{fullpage}
\usepackage{mdframed}
\begin{document}
\author{Adam J. Gray}
\title{Assignment 2}
\subject{Measure Theory}
\studentno{3329798}

\newcommand{\llra}{\Leftrightarrow}

\unswtitle

\section{}
\subsection{}
Let $ \mu $ and $ \nu $ be probability measures on $ ( \mathbb{R}, \mathcal{B}(\mathbb{R})) $.
We wish to show that the convolution
\begin{align}
	\mu * \nu(B) = \int\nu(B-x)\mu(dx)
\end{align}
of these two measures is well defined in the sense that $ \nu(B - x) $ is measurable in $ x $ and the integral exists.

Firstly see that
\begin{align}
	\nu(B-x) &= \int \chi_{B-x}(y) \nu(dy) \\
		&= \int \chi_{B}(x+y) \nu(dy)
\end{align}
and by seeing that $ \phi(x,y) = \chi_B(x+y) $ is $ \mathcal{B}(R) \otimes \mathcal{B}(R) $ measurable and so applying Tonelli's theorem $ \nu(B - x) $ is measurable.  

We now need to show that the integral exists. This is clear because
\begin{align}
	0 \leq \nu(B - x) \leq 1
\end{align}
and thus
\begin{align}
	\int \nu(B-x) \mu(dx) &\leq \int 1 \mu(dx) \\
		&= 1
\end{align}
because $ \mu $ is a probability measure. 

\subsection{}

Suppose there exists a bounded set $ F \in \mathcal{B}(\mathbb{R}) $ such that 
\begin{align}
	\mu * \nu(F) = 1 
\end{align}
we wish to show that there exists bounded sets $ G, H \in \mathcal{B}(\mathbb{R}) $ such that
\begin{align}
	\mu(G) = 1 \ \ \ \text{and} \ \ \ \nu(H) = 1. 
\end{align}
See that if 
\begin{align}
	1 &= \mu * \nu(F) \\
		&= \int \nu(F-x)\mu(dx) \\
		&= \int\int \chi_F(x+y) \nu(dy) \mu(dx)
\end{align}

Now as $ F $ is bounded there there exist intervals $ G = [a,b] $ and $ H = [c,d]$ such that
\begin{align}
	\int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy) \mu(dx)
\end{align}
and further
\begin{align}
	1=\int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) & \leq \int\int  \chi_G(x) \chi_H(y) \nu(dy) \mu(dx) \\
	&= \int \chi_{H}(y) \nu(dy) \int  \chi_G(x) \mu(dx)
\end{align}
\footnote{Note that the reason we could split up the integrals to a product in both cases is because $ \int \chi_H(y) \nu(y) $ is a constant with respect to $ x $.}
which implies that
\begin{align}
\int  \chi_G(x) \mu(dx) = 1 \ \ \ \text{ and } \ \ \ \int \chi_{H}(y) \nu(dy) = 1
\end{align}
that is $ \mu(G) = 1 $ and $ \nu(H) = 1 $.


Suppose that $ F $ is now countable but such that $ \mu * \nu (F) = 1 $.
Then
\begin{align}
	1 &= \int \nu(F - x) \mu(dx) \\
		&= \int \int \chi_{F}(x+y) \nu(dy) \mu(dx)
\end{align}
now since $ F = \left\{(x_k, y_k) \right\}_{k \in \Ntrl}$ is countable then there must exist countable sets $ G = \{ x_k \}_{k\in\Ntrl} $, $ H = \{ y_k \}_{k\in\Ntrl} $
such that
\begin{align}
	\int\int \chi_F(x+y) \nu(dy)\mu(dx) = \int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx)
\end{align}
and further
\begin{align}
	1 = \int\int \chi_F(x+y) \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) &\leq \int\int \chi_G(x) \chi_H(y) \nu(dy)\mu(dx) \\
	&= \int \chi_H(y) \nu(dy) \int \chi_G(x)\mu(dx)
\end{align}
which as before implies $ \mu(G) = 1 $ and $ \nu(H) = 1 $.

Then argument above also holds for finite $ F $ with exactly the same construction and so if $ F $ is finite and such that $ \mu * \nu(F) = 1 $ then there must exist finite sets $ G $ and $ H $ such that $ \mu(G) = 1 $ and $ \nu(H) = 1 $.
\section{}
\subsection{}
Suppose $ u $ and $ \nu $ are $\sigma$-finite \emph{positive} measures on $ (\Omega, \mathcal{F}) $. Then suppose that $ \mu << \nu $ and $ \nu << \mu $.
Then for $ A \in \mathcal{F} $ we have that $ \mu(a) = 0 \Rightarrow \nu(A) = 0 $ and $ \nu(A) = 0 \Rightarrow \mu(A) $. That is to say that $ \nu(A) =0 \Leftrightarrow \mu(A) = 0 $. That is to say that $ \nu $ and $ \mu $ have the same null sets. This argument is symmetric so it is clear that the reverse implication also holds.

We now wish to show that there is an $ \mathcal{F}$-measurable function $ g $ that satisfies $ 0 < g(\omega) < +\infty $ at each $ \omega \in \Omega $ and is such that $ \nu(A) = \int_Agd\mu $ for all $ A \in \mathcal{F} $.


Now suppose that there is a $ \mathcal{F} $ measurable function $ 0 < g(\omega) < \infty $ such that $ \nu(A) = \int_{A} g d\mu $.

Suppose $ \nu(A) =0 $ then
\begin{align}
	0 = \nu(A) &= \int_A g d\mu \\
	 &= \int \chi_A g d\mu
\end{align}
Now as $ \chi_A g >= 0 $ and $ \mu $ is a positive measure then by theorem 2.18 (3) we must that $ \chi_Ag = 0 $, $ \mu-$a.e. As $ g > 0 $ we have that $ \chi_A = 0 $ $ \mu$-a.e. which means that $ \mu(A) = 0 $.
So $ \mu \ll \nu $.

We claim that $ \nu \ll \mu $ as well. Suppose $ \mu(A) =0 $ then note that as $ g $ is a measurable function we have that by Theorem 2.11 $ g(\omega) = \lim_{n\lra\infty} s_n(\omega) $ for all $ \omega $ and a sequence of increasing simple functions.
That is
\begin{align}
	\int_A g d\mu = \int_A \lim_{n\lra\infty} s_n d\mu.
\end{align}
Further by the monotone convergence theorem we have that 
\begin{align}
\int_A \lim_{n\lra\infty} s_n d\mu = \lim_{n\lra\infty}\int_A s_n d\mu.
\end{align}

Now for some simple function $ s $ we have that if $ \mu(A) =0 $ then 
\begin{align}
	\int_A s d\mu &= \int_A\sum_{k=1}^{N} \alpha_k \chi_{B_k} d\mu \\
		&= \sum_{k=1}^N \alpha_k \int_A \chi_{B_k} d\mu \\
		&= \sum_{k=1}^N \alpha_k \int \chi_{B_k\cap A} d\mu \\
		&\leq  \sum_{k=1}^N \alpha_k \int \chi_{A} d\mu \\
		&= \sum_{k=1}^N \alpha_k 0\\
		&= 0
\end{align}
thus if $ \mu(A) = 0 $ then $ \int_A s_n d\mu =0 $ for all $ n $ and so $ \lim_{n \lra\infty} \int_A s_n d\mu = 0$ and hence $ \nu(A) =0 $. This means that $ \nu \ll \mu $ as well.

\subsection{}
Let $ \{ B_n \}_{n \in \Ntrl} $ be a covering of $ \Omega $ by disjoint sets with $ 0 < \mu(B_n) < \infty $ for all $ n $. Such a covering exists because $ \mu $ is $\sigma$-finite. Now select a sequence of constants $ \{ \alpha_n \}_{n \in \Ntrl} $ such that $ \alpha_n > 0 $ for all $ n $ and 
\begin{align}
	\sum_{n=1}^\infty \alpha_n = 1.
\end{align}
For example we could select  $ \alpha_n = 2^{-n} $.

We claim that the function $ \nu : \mathcal{F} \lra \mathbb{R} $ defined by
\begin{align}
	\nu(A) = \sum_{n=1}^\infty \alpha_n \frac{\mu(A \cap B_n)}{\mu(B_n)}
\end{align}
is a probability measure with the same null sets as $ \mu $.
Firstly we show that it is a measure, that is we show $\sigma$-additivity.

For a collection of disjoint sets $ \{ A_n \}_{n \in \Ntrl} $ we have that
\begin{align}
	\nu\left( \bigcup_{n\in\Ntrl} A_n \right) &= \sum_{k=1}^\infty \alpha_k \frac{\mu\left( \bigcup_{n=1}^\infty A_n \cap B_k \right)}{\mu(B_k)}
\end{align}
and by $ \sigma$-additivity of $ \mu $ we get
\begin{align}
	\sum_{k=1}^\infty \alpha_k \frac{\mu\left( \bigcup_{n=1}^\infty A_n \cap B_k \right)}{\mu(B_k)} &= \sum_{k=1}^\infty \alpha_k \frac{\sum_{n=1}^\infty \mu\left( A_n \cap B_k \right)}{\mu(B_k)} \\
	&= \sum_{n=1}^\infty  \sum_{k=1}^\infty \alpha_k \frac{\mu\left( A_n \cap B_k \right)}{\mu(B_k)} \\
	&= \sum_{n=1}^\infty \nu(A_n).
\end{align}
The interchange of the order of summation can be justified by the fact that
$ \mu(A_n \cap B_k) \geq 0 $ and 
\begin{align}
\sum_{k=1}^\infty \alpha_k \frac{\sum_{n=1}^\infty \mu\left( A_n \cap B_k \right)}{\mu(B_k)} < \infty
\end{align}
which we prove now (by proving $ \nu $ is a probability measure).

See that
\begin{align}
	\nu(\Omega) &= \sum_{k=1}^\infty \alpha_k \frac{\mu(\Omega\cap B_k)}{\mu(B_k)} \\
	&= \sum_{k=1}^\infty \alpha_k \frac{\mu( B_k)}{\mu(B_k)} \\
	&= \sum_{k=1}^\infty \alpha_k \\
	&= 1.
\end{align}
We now just have to show that $ \mu $ and $ \nu $ share the same null sets. 

Suppose $ \mu(A) = 0 $ then
\begin{align}
	\mu(A) &= \sum_{k=1}^\infty \alpha_k \frac{\mu(A \cap B_k)}{\mu(B_k)} \\
		&\leq \sum_{k=1}^\infty \alpha_k \frac{\mu(A)}{\mu(B_k)} \\
		&= 0
\end{align}
and thus $ \nu \ll \mu $.
Now suppose $ \nu(A) =0 $ then
\begin{align}
	\sum_{k=1}^\infty \alpha_k \frac{\mu(A \cap B_k)}{\mu(B_k)} = 0
\end{align}
and as $ \mu(B_k) < \infty $ for all $ k $ this implies that $ \mu(A \cap B_k ) = 0$ for all $ k $. Now
\begin{align}
	0 &= \sum_{k=1}^\infty \mu(A \cap B_k) \\
		&= \mu\left( \bigcup_{k=1}^\infty (A \cap B_k) \right) \\
		&= \mu\left( A \cap \bigcup_{k=1}^\infty B_k\right) \\
		&= \mu(A \cap \Omega) \\
		&= \mu(A).
\end{align}
Thus $ \mu \ll \nu $.

So $ \nu $ is a finite measure on $ (\Omega, \mathcal{F}) $ which is equivalent to $ \mu $.
\section{}
\subsection{}
Firstly note that the definition of the characteristic function is
\begin{align}
	\hat{\mu}_X(u) = \mathbb{E}[\exp(i\langle X, u \rangle)]
\end{align}
and so for the random vector $ cX $ with $ c \in \Rl $ we have that
\begin{align}
	\hat{\mu}_{cX}(u) &= \mathbb{E}[ \exp(i \langle cX, u \rangle) ] \\
		 &= \mathbb{E}[ \exp(i \langle X, cu \rangle) ] \\
		 &= \hat{\mu}_{X}(cu)
\end{align}
\subsection{}
By definition we have that
\begin{align}
	\hat{\mu}(\mathbf{u}) = \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We can say that
\begin{align}
	\frac{d^\alpha \hat{\mu}(\mathbf{u})}{d \mathbf{u}^\alpha} = \frac{d^\alpha}{d \mathbf{u}^\alpha} \int \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}).
\end{align}
We wish to justify taking this derivative through the integral sign. To do this we use an extension of corollary 2.28 (2) from the notes.
\clearpage
\begin{mdframed}
\textbf{Claim}

If the partial derivative 
\begin{align}
	\frac{\partial^\alpha}{\partial\mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u})
\end{align}
exists for all $ (\mathbf{x}, \mathbf{u}) \in X \times [a, b]^d $ and if there is a function $ g \in L^{1}(\mu) $ such that
\begin{align}
	\Big| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) \Big| \leq g
\end{align}
for every $ \mathbf{x} \in X $ and $ \mathbf{u} \in (a,b)^d $ then
\begin{align}
	\frac{d^\alpha}{d\mathbf{u}^\alpha} \int f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) = \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} f(\mathbf{x}, \mathbf{u}) d\mu(\mathbf{x}) \ \ \ \ \ \text{ for } \mathbf{u} \in (a,b)^d.
\end{align}
The proof follows from induction on the dimension of $ \mathbf{x} $ and $ \mathbf{u} $ and the order of the derivative. The base case is specifically the statement of  corollary 2.28 (2) from the notes.
\end{mdframed}

We have that 
\begin{align}
	\frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) = i^{|\alpha|}\prod_{k=1}^d x_k^{\alpha_k}  \exp(i \langle \mathbf{x}, \mathbf{u} \rangle )
\end{align}
and that
\begin{align}
	\left| \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) \right| \leq  \underbrace{\prod_{k=1}^d \left| x_k \right|
^{\alpha_k}}_{\circledast}.
\end{align}
Now because 
\begin{align}
	\mathbf{E}\left( \prod_{k=1}^d |X_k|^{\alpha_k}\right) = \int \prod_{k=1}^d \left| x_k \right|
^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) < \infty 
\end{align}
then $ \circledast \in L^1(\mathbb{P}_X) $
and so we can apply our claim (the DCT) to get
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} &=  \int \frac{\partial^\alpha}{\partial \mathbf{u}^\alpha} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= \int i^{|\alpha|} \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{u} \rangle ) d\mathbb{P}_{X}(\mathbf{x})
\end{align}
and so
\begin{align}
	\frac{\partial^\alpha \hat{\mu}(\mathbf{u})}{\partial \mathbf{u}^\alpha} \Big|_{\mathbf{u} = \mathbf{0}} &= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} \exp(i \langle \mathbf{x}, \mathbf{0} \rangle ) d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \int  \prod_{k=1}^d x_k^{\alpha_k} d\mathbb{P}_{X}(\mathbf{x}) \\
	&= i^{|\alpha|} \mathbb{E}(X^\alpha)
\end{align}
\subsection{}
Let $ d = 1 $ and let $ \mu $ have the  Lebesgue density,
\begin{align}
	f(x) = \frac{C}{(1+x^2) \log(e + x^2)}, \ \ \ \ x \in \Rl.
\end{align}
We wish to show that $ E[X] $ is not defined but $ \hat{\mu}(u) $ is differentiable at $ 0 $. Firstly we show that $ E[X] $ is not defined. 
If $ \mathbb{E}(X) $ were defined then
\begin{align}
	\mathbb{E}(X) &= \int \frac{xC}{(1+x^2)\log(e + x^2)} dx \\
		&=  \underbrace{\int_{-\infty}^0 \frac{xC}{(1+x^2)\log(e + x^2)} dx}_{\mathbb{E}(X^-)} + \underbrace{\int_0^{\infty} \frac{xC}{(1+x^2)\log(e + x^2)} dx}_{\mathbb{E}(X^+)}
\end{align}
but
\begin{align}
	\int_0^{\infty} \frac{xC}{(1+x^2)\log(e + x^2)} dx &\geq \int_1^{\infty} \frac{xC}{2x^2\log(e + x^2)} dx \\
	&= \int_1^{\infty} \frac{C}{2x\log(e + x^2)} dx \\
	&\geq \int_4^{\infty} \frac{C}{2x\log(4x^2)} dx \\
	&\geq \int_6^{\infty} \frac{C}{10x\log(x)} dx.
\end{align}
Let $ u = \log(x) $ so that $ du = \frac{1}{x} dx $
and so 
\begin{align}
	\int_6^{\infty} \frac{C}{10x\log(x)} dx = \underbrace{\int_{\log(6)}^\infty \frac{C}{10u} du}_{\circledast}
\end{align}
and $ \circledast $ diverges. So $ \mathbb{E}(X^+) $ does not exist and in a similar manner we can see that $ \mathbb{E}(X^-) $ does not exist and thus $ E(X) $ is not defined.

However we can calculate $ \hat{\mu}(u) $ as
\begin{align}
	\hat{\mu}(u) = \int \frac{e^{ixu}C}{(1+x^2)\log(e + x^2)}dx
\end{align}
and note that $ \hat{\mu}(u) $ is differentiable at $ 0 $ if the following limit exists
\begin{align}
	\lim_{h \lra 0} \int \frac{e^{ixh}C - C}{h(1+x^2)\log(e + x^2)}dx
\end{align}
\section{}
Let $\mu$ be the binomial distribution with $ n $ trials and probability of success $ p $, that is $ \mu = \operatorname{Bin}(n,p) $, and let $ \nu $ be the Poisson distribution with mean $ \lambda > 0 $. 
\subsection{}
We wish to verify that $ \hat{\mu}(u) = (1 - p + pe^{iu})^n $. 
Because the binomial distribution is just the convolution of identical independent Bernoulli distributions then we just have to verify that $ (1 - p + pe^{iu} $ is the characteristic function for $Bernoulli(p)$.

If $ \nu $ is the Bernoulli measure and $ X $ has law $ \nu $ then 
\begin{align}
	\hat{\nu}(u) &= \mathbb{E}[ \exp(iuX)] \\
		&= \sum_{k \in \{ 0, 1\}} e^{iuk} \nu_{X}(k) \\
		&=  pe^{iu} + (1 - p).
\end{align}
Then by repeated application of the convolution theorem we get that
$ \hat{\mu}(u) = (1 - p + pe^{iu})^n $.

\subsection{}
We wish to verify that $ \hat{\nu}(u) = \exp(\lambda(e^{iu} - 1)) $. 
The probability mass function of the Poisson distribution is 
\begin{align}
	\frac{\lambda^k}{k!} e^{-\lambda} 
\end{align}
and thus
\begin{align}
	\mathbb{E}[ \exp(iuX) ] &= \sum_{k=0}^\infty \frac{\lambda^k}{k!} e^{-\lambda} e^{iuk} \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!} (e^{iu})^k \\
		&= e^{-\lambda}\sum_{k=0}^\infty \frac{(\lambda e^{iu})^k}{k!} \\
		&= e^{-\lambda} e^{\lambda e^{iu}} \\
		&= e^{\lambda( e^{iu} - 1)}
\end{align}

\subsection{}

We wish to show that if $ p_n $ is a sequence in $ [0,1] $ such that $ p_n \downarrow 0 $ and $ np_n \lra \lambda $ then $ \mu_n \lra \nu $ in the weak sense where $ \mu_n = \operatorname{Bin}(n, p_n) $.
Let $ f \in C_b $ then
\begin{align}
	\lim_{n\lra \infty} \sum_{k=0}^{n} f(k) {n\choose k} p_n^k(1-p_n)^{n-k}
		&= \lim_{n\lra \infty} \sum_{k=0}^\infty \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} \\
		&= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k}.
\end{align}
The interchange of the order of the limit and the sum is justified by the uniform convergence of the sum. To see this let $ M = \sup_{k \in \Ntrl^0} f(k) $ (which exists because $ f \in C_b $) and then note that 
\begin{align}
	\sum_{k=0}^\infty \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} &\leq \sum_{k=0}^\infty \chi_{k \leq n} \cdot M {n\choose k} p_n^k(1-p_n)^{n-k} \\
	&= M < \infty
\end{align}
and so by the Weierstrass M test the series converges uniformly. 

Now as $ np_n \lra \lambda $ or $ p_n \lra \frac{\lambda}{n} $ we get
\begin{align}
	\sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) {n\choose k} p_n^k(1-p_n)^{n-k} &= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) \frac{n!}{k!(n-k)!} p_n^k(1-p_n)^{n} (1-p_n)^{-k} \\
	&= \sum_{k=0}^\infty \lim_{n\lra \infty} \chi_{k \leq n} \cdot f(k) \frac{n^k + O(n^{k-1})}{k!} p_n^k(1-p_n)^{n} (1-p_n)^{-k} \\
	&= \sum_{k=0}^\infty  f(k)\lim_{n\lra \infty} \frac{n^k + O(n^{k-1})}{k!} p_n^k(1-p_n)^{n} \underbrace{(1-p_n)^{-k}}_{\lra 0} \\
	&= \sum_{k=0}^\infty  f(k)\lim_{n\lra \infty} \underbrace{\frac{n^k + O(n^{k-1})}{k!} p_n^k}_{\lra \frac{\lambda^k}{k!}}(1-p_n)^{n} \\
	&= \sum_{k=0}^\infty \frac{\lambda^k}{k!} f(k)\lim_{n\lra \infty} \underbrace{(1-\frac{\lambda}{n})^{n}}_{\lra e^{-\lambda}} \\
	&= \sum_{k=0}^\infty \frac{\lambda^k}{k!} e^{-\lambda} f(k).
\end{align}
This proves the weak convergence.
\subsection{}
This argument holds whether one takes the integral (sum) or not. So $ \mu_n(\{k\}) \lra \nu(\{k\}) $ for all $ k \in \Ntrl^0$.
\section{}
\subsection{}
We wish to show that $ \mathbb{P}(B_n) = 1/2 $ for every $ n \geq 1 $.
Note that 
\begin{align}
	B_n = \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big)
\end{align}
and thus
\begin{align}
	\mathbb{P}(B_n) &= \mathbb{P}\left( \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \mathbb{P}\left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
		&= \sum_{k=0}^{2^{n-1}-1} \frac{1}{2^n} \\
		&= 2^{n-1} \frac{1}{2^n} \\
		&= \frac{1}{2}.
\end{align}
\subsection{}
We now wish to show that the sequence of events $ B_n $ form an infinite sequence of independent events. 

Take a finite subset $ J \subset \Ntrl $ with $ |J| = m $ and $ \max J = r $ then
\begin{align}
	\mathbb{P}\left( \bigcap_{n \in J} B_n \right) &= \mathbb{P}\left( \bigcap_{n \in J} \bigcup_{k=0}^{2^{n-1} - 1} \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= \mathbb{P} \left( \bigcup_{k=0}^{2^{r-m} - 1} \Big[ \frac{2k}{2^r}, \frac{2k+1}{2^r} \Big)\right) \\
	&=  \sum_{k=0}^{2^{r-m} - 1} \mathbb{P} \left( \Big[ \frac{2k}{2^n}, \frac{2k+1}{2^n} \Big) \right) \\
	&= 2^{r-m} \frac{1}{2^r} \\
	&= \frac{1}{2^m} \\
	&= \prod_{n \in J} \mathbb{P}\left( B_n \right)
\end{align}
and so the sequence of events $ B_n $ form an infinite sequence of independent events.
\subsection{}

We wish to show / argue that the probability that a randomly sampled number $ \omega $ will have the sequence $ 5825 $ occur infinitely often in its decimal expansion is 1.

We use the Borel-Cantelli lemma. Ignoring possible overlaps (on the $5$s) we can see that we can break any decimal expansion of $ \omega $ up into blocks of $ 4 $ digits. 

Then by we can define $ E_i $ as the probability of obtaining $ 5285 $ in the i-th block possition. By the same argument as above these events are independent.

The for any $ i $ we have that $ \mathbb{P}(E_i) = \frac{1}{10000} $ (the same argument as above applied to a decimal expansion). Then clearly
\begin{align}
  \mathbb{P}( E_i ) = \infty.
\end{align}

By the Borel-Cantelli lemma this implies
\begin{align}
  \mathbb{P}( \limsup_n( E_n ) = 1.
\end{align}

Now 
\begin{align}
  \limsup_n( E_n ) = \bigcap_{n=1}^\infty \bigcup_{j=n}^\infty E_j 
\end{align}
can be intuatively read as $ E_j $ happens infinitely often. Which is to say that $ 5285 $ occurs \emph{blockwise} in the expansion of $ \omega $ infinitely often. Clearly as allowing for overlaps allows for more configurations then the probability is $ 1 $ (it can be no more). 
\end{document}